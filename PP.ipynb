{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with_mask_1.jpg', 'with_mask_10.jpg', 'with_mask_100.jpg', 'with_mask_1000.jpg', 'with_mask_1001.jpg']\n",
      "['with_mask_995.jpg', 'with_mask_996.jpg', 'with_mask_997.jpg', 'with_mask_998.jpg', 'with_mask_999.jpg']\n"
     ]
    }
   ],
   "source": [
    "with_mask_files = os.listdir(\"C:/Users/user/OneDrive/Desktop/DataScience/Machine Learning A-Z (Codes and Datasets)/archive (7)\\data/with_mask\")\n",
    "print(with_mask_files[0:5])\n",
    "print(with_mask_files[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['without_mask_1.jpg', 'without_mask_10.jpg', 'without_mask_100.jpg', 'without_mask_1000.jpg', 'without_mask_1001.jpg']\n",
      "['without_mask_995.jpg', 'without_mask_996.jpg', 'without_mask_997.jpg', 'without_mask_998.jpg', 'without_mask_999.jpg']\n"
     ]
    }
   ],
   "source": [
    "without_mask_files = os.listdir(\"C:/Users/user/OneDrive/Desktop/DataScience/Machine Learning A-Z (Codes and Datasets)/archive (7)/data/without_mask\")\n",
    "print(without_mask_files[0:5])\n",
    "print(without_mask_files[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of with mask images: 3725\n",
      "Number of without mask images: 3828\n"
     ]
    }
   ],
   "source": [
    "print('Number of with mask images:', len(with_mask_files))\n",
    "print('Number of without mask images:', len(without_mask_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lables for the two class of images\n",
    "#with mask  -->  1\n",
    "#without mask  -->  0\n",
    "# create the labels\n",
    "\n",
    "with_mask_labels = [1]*3725\n",
    "\n",
    "without_mask_labels = [0]*3828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(with_mask_labels[0:5])\n",
    "\n",
    "print(without_mask_labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3725\n",
      "3828\n"
     ]
    }
   ],
   "source": [
    "print(len(with_mask_labels))\n",
    "print(len(without_mask_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7553\n",
      "[1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "labels = with_mask_labels + without_mask_labels\n",
    "\n",
    "print(len(labels))\n",
    "print(labels[0:5])\n",
    "print(labels[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\CONDA\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data shape: (7553, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Define the paths to your images\n",
    "with_mask_path = 'C:/Users/user/OneDrive/Desktop/DataScience/Machine Learning A-Z (Codes and Datasets)/archive (7)/data/with_mask'\n",
    "without_mask_path = 'C:/Users/user/OneDrive/Desktop/DataScience/Machine Learning A-Z (Codes and Datasets)/archive (7)/data/without_mask'\n",
    "\n",
    "# Initialize an empty list to store all images\n",
    "all_data = []\n",
    "\n",
    "# Define the ImageDataGenerator with the desired transformations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Helper function to process images\n",
    "def process_images(image_path):\n",
    "    data = []\n",
    "    image_files = os.listdir(image_path)\n",
    "    for img_file in image_files:\n",
    "        image = Image.open(os.path.join(image_path, img_file))\n",
    "        image = image.resize((128, 128))\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # Convert image to numpy array\n",
    "        image_array = np.array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)  \n",
    "        # Apply the transformations\n",
    "        transformed_image = train_datagen.flow(image_array, batch_size=1)[0]\n",
    "        \n",
    "        # Remove the batch dimension and add to the list\n",
    "        data.append(transformed_image[0])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Process and add \"with mask\" images\n",
    "all_data.extend(process_images(with_mask_path))\n",
    "\n",
    "# Process and add \"without mask\" images\n",
    "all_data.extend(process_images(without_mask_path))\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "all_data = np.array(all_data)\n",
    "\n",
    "# Example: Print the shape of the resulting array\n",
    "print('All data shape:', all_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7553"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.04988803, 0.03443858, 0.01747746],\n",
       "        [0.04229958, 0.02844304, 0.02032941],\n",
       "        [0.12305809, 0.07855204, 0.0480517 ],\n",
       "        ...,\n",
       "        [0.21782398, 0.21043499, 0.10003355],\n",
       "        [0.22229491, 0.21198335, 0.09385407],\n",
       "        [0.20515639, 0.21406518, 0.10887672]],\n",
       "\n",
       "       [[0.07175341, 0.05604774, 0.03636488],\n",
       "        [0.06557968, 0.04440612, 0.03027485],\n",
       "        [0.11735173, 0.06917755, 0.04545033],\n",
       "        ...,\n",
       "        [0.20147206, 0.18335374, 0.07139201],\n",
       "        [0.23293874, 0.21038482, 0.08776405],\n",
       "        [0.23610659, 0.24032591, 0.12525058]],\n",
       "\n",
       "       [[0.13437504, 0.09809285, 0.05073687],\n",
       "        [0.08214957, 0.05506284, 0.02495655],\n",
       "        [0.10188457, 0.06047384, 0.03705439],\n",
       "        ...,\n",
       "        [0.19654404, 0.16905473, 0.05880686],\n",
       "        [0.23859343, 0.2029547 , 0.08178215],\n",
       "        [0.25251728, 0.24336636, 0.12837453]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.11095882, 0.12222651, 0.07574847],\n",
       "        [0.10626648, 0.10773921, 0.069474  ],\n",
       "        [0.09754927, 0.10090394, 0.07073673],\n",
       "        ...,\n",
       "        [0.27637225, 0.3051755 , 0.20275255],\n",
       "        [0.3089748 , 0.3351628 , 0.22185089],\n",
       "        [0.35329896, 0.37948698, 0.26543885]],\n",
       "\n",
       "       [[0.15325484, 0.1700725 , 0.12784316],\n",
       "        [0.16560286, 0.18375006, 0.15080589],\n",
       "        [0.11752214, 0.12959574, 0.10206151],\n",
       "        ...,\n",
       "        [0.2810098 , 0.32315734, 0.2240085 ],\n",
       "        [0.31932777, 0.3597787 , 0.24904095],\n",
       "        [0.3335525 , 0.3731375 , 0.25504613]],\n",
       "\n",
       "       [[0.15133074, 0.17863305, 0.14090022],\n",
       "        [0.1768335 , 0.22087906, 0.19099754],\n",
       "        [0.13712162, 0.16368453, 0.13233113],\n",
       "        ...,\n",
       "        [0.28262734, 0.335437  , 0.23949343],\n",
       "        [0.31471556, 0.36734885, 0.25815037],\n",
       "        [0.29942182, 0.34963575, 0.22894408]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting image list and label list to numpy arrays\n",
    "\n",
    "X = np.array(all_data)\n",
    "Y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7553, 128, 128, 3)\n",
      "(7553,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7553, 128, 128, 3) (6042, 128, 128, 3) (1511, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data\n",
    "\n",
    "X_train_scaled = X_train/255\n",
    "\n",
    "X_test_scaled = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\CONDA\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "num_of_classes = 2\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(128,128,3)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(num_of_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the neural network\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 290ms/step - acc: 0.4997 - loss: 0.6937 - val_acc: 0.5140 - val_loss: 0.6930\n",
      "Epoch 2/5\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 294ms/step - acc: 0.5109 - loss: 0.6932 - val_acc: 0.5140 - val_loss: 0.6928\n",
      "Epoch 3/5\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 291ms/step - acc: 0.4952 - loss: 0.6934 - val_acc: 0.5140 - val_loss: 0.6930\n",
      "Epoch 4/5\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 260ms/step - acc: 0.4947 - loss: 0.6933 - val_acc: 0.5140 - val_loss: 0.6928\n",
      "Epoch 5/5\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 293ms/step - acc: 0.5109 - loss: 0.6930 - val_acc: 0.5140 - val_loss: 0.6929\n"
     ]
    }
   ],
   "source": [
    "# training the neural network\n",
    "history = model.fit(X_train_scaled, Y_train, validation_split=0.1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = input('Path of the image to be predicted: ')\n",
    "\n",
    "input_image = cv2.imread(input_image_path)\n",
    "\n",
    "cv2_imshow(input_image)\n",
    "\n",
    "input_image_resized = cv2.resize(input_image, (128,128))\n",
    "\n",
    "input_image_scaled = input_image_resized/255\n",
    "\n",
    "input_image_reshaped = np.reshape(input_image_scaled, [1,128,128,3])\n",
    "\n",
    "input_prediction = model.predict(input_image_reshaped)\n",
    "\n",
    "print(input_prediction)\n",
    "\n",
    "\n",
    "input_pred_label = np.argmax(input_prediction)\n",
    "\n",
    "print(input_pred_label)\n",
    "\n",
    "\n",
    "if input_pred_label == 1:\n",
    "\n",
    "  print('The person in the image is wearing a mask')\n",
    "\n",
    "else:\n",
    "\n",
    "  print('The person in the image is not wearing a mask')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
